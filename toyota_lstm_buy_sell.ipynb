{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a991cf",
   "metadata": {},
   "source": [
    "\n",
    "# トヨタ株 (7203.T) — 翌日終値予測 + 売買判断（Buy/Sell）\n",
    "**目的**: 2020/09〜2025/09 の5年間の株価データを用い、  \n",
    "1) 翌日の終値を**回帰**で予測し、  \n",
    "2) その予測に基づいて**売買判断（Buy/Sell）**の**分類**を行う。\n",
    "\n",
    "**ポイント**  \n",
    "- クリプト向けチュートリアル（15分足 / GMO Fetcher）を **日次株価 / Yahoo Finance 取得**に置換。  \n",
    "- `GmoFetcher` 相当の薄いラッパー (`YfFetcher`) を実装し、**キャッシュ（joblib.Memory）**で再取得を抑制。  \n",
    "- **LSTM** を用いた時系列ウィンドウ学習（回帰）＋ 予測結果を使った **2値分類**。  \n",
    "- ベースライン（線形回帰）と比較、さらに**指標可視化**、**単純バックテスト**を実装。\n",
    "\n",
    "> 環境にネットアクセスが無い場合は、Kaggle等からダウンロードした `7203.T.csv` を読み込むパスも提供します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88e705fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.20.0\n",
      "HAS_YF: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# === Imports ===\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from joblib import Memory\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ML/DL\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Optional: yfinance for data download (requires internet)\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    HAS_YF = True\n",
    "except Exception:\n",
    "    HAS_YF = False\n",
    "\n",
    "print('TensorFlow:', tf.__version__)\n",
    "print('HAS_YF:', HAS_YF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "29300008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Parameters ===\n",
    "TICKER = \"TM\"      # トヨタ自動車 (NYSE: TM)\n",
    "START_DATE = \"2020-09-01\"\n",
    "END_DATE   = \"2025-09-30\"\n",
    "\n",
    "CACHE_DIR = Path('/tmp/yf_cache_v2')\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "memory = Memory(location=str(CACHE_DIR), verbose=0)\n",
    "\n",
    "# 学習ハイパラ\n",
    "WINDOW_SIZE = 30     # 何日分の履歴で翌日を予測するか\n",
    "BATCH_SIZE  = 64\n",
    "EPOCHS      = 50\n",
    "VAL_SPLIT   = 0.0     # 明示的に時系列分割するので 0\n",
    "\n",
    "# 時系列分割（固定境界）\n",
    "SPLIT_TRAIN_END = \"2024-03-31\"\n",
    "SPLIT_VAL_END   = \"2025-03-31\"  # val: 2024-04-01〜2025-03-31\n",
    "# test: 2025-04-01〜2025-09-30\n",
    "\n",
    "# 乱数シード\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a6e52e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === YfFetcher: GmoFetcher 相当の簡易ラッパ ===\n",
    "def _download_ohlcv_v2(ticker, start, end, interval='1d', csv_path=None):\n",
    "    if csv_path is not None and Path(csv_path).exists():\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # CSV 形式: 先頭行に列名、2行目がティッカーのみの場合があるので除外\n",
    "        candidate_date_cols = [c for c in df.columns if str(c).strip().lower() in {'date', 'datetime'}]\n",
    "        if candidate_date_cols:\n",
    "            date_col = candidate_date_cols[0]\n",
    "            df = df.dropna(subset=[date_col])\n",
    "            df[date_col] = pd.to_datetime(df[date_col])\n",
    "            df = df.set_index(date_col)\n",
    "        else:\n",
    "            # 先頭列が日付と想定\n",
    "            first_col = df.columns[0]\n",
    "            df = df.dropna(subset=[first_col])\n",
    "            df[first_col] = pd.to_datetime(df[first_col])\n",
    "            df = df.set_index(first_col)\n",
    "        return df\n",
    "\n",
    "    if not HAS_YF:\n",
    "        raise RuntimeError(\"yfinance が利用できません。csv_path を指定してください。\")\n",
    "\n",
    "    return yf.download(ticker, start=start, end=end, interval=interval, progress=False)\n",
    "\n",
    "\n",
    "class YfFetcher:\n",
    "    def __init__(self, memory=None):\n",
    "        self.memory = memory\n",
    "        if memory is not None:\n",
    "            # メモリキャッシュは関数定義が変わると壊れるのでバージョン付きで保持\n",
    "            self._fetch_fn = memory.cache(_download_ohlcv_v2)\n",
    "        else:\n",
    "            self._fetch_fn = _download_ohlcv_v2\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_df(df):\n",
    "        # yfinance の列名を統一し、DatetimeIndexへ\n",
    "        # (Open High Low Close Adj Close Volume)\n",
    "        if df is None or len(df) == 0:\n",
    "            return pd.DataFrame()\n",
    "        df = df.copy()\n",
    "        if not isinstance(df.index, pd.DatetimeIndex):\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        df = df.dropna(axis=0, how='any', subset=[df.index.name] if df.index.name else None)\n",
    "        df = df.sort_index()\n",
    "        if hasattr(df.columns, 'nlevels') and df.columns.nlevels > 1:\n",
    "            df.columns = [col if isinstance(col, str) else col[0] for col in df.columns]\n",
    "        # 列名を小文字に揃える\n",
    "        cols = {c: str(c).lower().replace(' ', '').replace('_', '') for c in df.columns}\n",
    "        df = df.rename(columns=cols)\n",
    "        # 必須列があるかチェック\n",
    "        must = ['open', 'high', 'low', 'close', 'adjclose', 'volume']\n",
    "        for m in must:\n",
    "            if m not in df.columns:\n",
    "                # adjclose がないケースもあるので Close を複製\n",
    "                if m == 'adjclose' and 'close' in df.columns:\n",
    "                    df['adjclose'] = df['close']\n",
    "                else:\n",
    "                    raise ValueError(f\"Missing column: {m}\")\n",
    "        return df[['open','high','low','close','adjclose','volume']]\n",
    "\n",
    "    def fetch_ohlcv(self, ticker, start, end, interval='1d', csv_path=None):\n",
    "        \"\"\"\n",
    "        ticker: 例 '7203.T'\n",
    "        start, end: 'YYYY-MM-DD'\n",
    "        interval: '1d' 固定（株は日次で扱う）\n",
    "        csv_path: ローカルCSVのパス（ネット不可時）\n",
    "        \"\"\"\n",
    "        raw_df = self._fetch_fn(ticker, start, end, interval, csv_path)\n",
    "        return self._normalize_df(raw_df)\n",
    "\n",
    "\n",
    "fetcher = YfFetcher(memory=memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "58ff2403",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing column provided to 'parse_dates': 'Date'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[34]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m START_DATE = \u001b[33m\"\u001b[39m\u001b[33m2020-09-01\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m END_DATE = \u001b[33m\"\u001b[39m\u001b[33m2025-09-30\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df = \u001b[43mfetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_ohlcv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mticker\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTICKER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m=\u001b[49m\u001b[43mSTART_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEND_DATE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m1d\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mCSV_PATH\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ここを指定すればYahoo非依存\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.head())\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(df.tail())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 53\u001b[39m, in \u001b[36mYfFetcher.fetch_ohlcv\u001b[39m\u001b[34m(self, ticker, start, end, interval, csv_path)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_ohlcv\u001b[39m(\u001b[38;5;28mself\u001b[39m, ticker, start, end, interval=\u001b[33m'\u001b[39m\u001b[33m1d\u001b[39m\u001b[33m'\u001b[39m, csv_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m     47\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[33;03m    ticker: 例 '7203.T'\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[33;03m    start, end: 'YYYY-MM-DD'\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[33;03m    interval: '1d' 固定（株は日次で扱う）\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    csv_path: ローカルCSVのパス（ネット不可時）\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     raw_df = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._normalize_df(raw_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/memory.py:607\u001b[39m, in \u001b[36mMemorizedFunc.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    605\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    606\u001b[39m     \u001b[38;5;66;03m# Return the output, without the metadata\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cached_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelving\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/memory.py:562\u001b[39m, in \u001b[36mMemorizedFunc._cached_call\u001b[39m\u001b[34m(self, args, kwargs, shelving)\u001b[39m\n\u001b[32m    556\u001b[39m     \u001b[38;5;28mself\u001b[39m.warn(\n\u001b[32m    557\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mComputing func \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, argument hash \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    558\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min location \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocation\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    559\u001b[39m     )\n\u001b[32m    561\u001b[39m \u001b[38;5;66;03m# Returns the output but not the metadata\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshelving\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/joblib/memory.py:832\u001b[39m, in \u001b[36mMemorizedFunc._call\u001b[39m\u001b[34m(self, call_id, args, kwargs, shelving)\u001b[39m\n\u001b[32m    830\u001b[39m \u001b[38;5;28mself\u001b[39m._before_call(args, kwargs)\n\u001b[32m    831\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    833\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._after_call(call_id, args, kwargs, shelving, output, start_time)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36m_download_ohlcv_v2\u001b[39m\u001b[34m(ticker, start, end, interval, csv_path)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_download_ohlcv_v2\u001b[39m(ticker, start, end, interval=\u001b[33m'\u001b[39m\u001b[33m1d\u001b[39m\u001b[33m'\u001b[39m, csv_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m      3\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m csv_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m Path(csv_path).exists():\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m         df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDate\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m         df = df.set_index(\u001b[33m'\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1898\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1897\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1898\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1899\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1900\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/c_parser_wrapper.py:161\u001b[39m, in \u001b[36mCParserWrapper.__init__\u001b[39m\u001b[34m(self, src, **kwds)\u001b[39m\n\u001b[32m    155\u001b[39m         \u001b[38;5;28mself\u001b[39m._validate_usecols_names(\n\u001b[32m    156\u001b[39m             usecols,\n\u001b[32m    157\u001b[39m             \u001b[38;5;28mself\u001b[39m.names,  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    158\u001b[39m         )\n\u001b[32m    160\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_parse_dates_presence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnames\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[32m    162\u001b[39m \u001b[38;5;28mself\u001b[39m._set_noconvert_columns()\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/io/parsers/base_parser.py:243\u001b[39m, in \u001b[36mParserBase._validate_parse_dates_presence\u001b[39m\u001b[34m(self, columns)\u001b[39m\n\u001b[32m    233\u001b[39m missing_cols = \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\n\u001b[32m    234\u001b[39m     \u001b[38;5;28msorted\u001b[39m(\n\u001b[32m    235\u001b[39m         {\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     )\n\u001b[32m    241\u001b[39m )\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m missing_cols:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    244\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing column provided to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mparse_dates\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    245\u001b[39m     )\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    248\u001b[39m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[32m    250\u001b[39m ]\n",
      "\u001b[31mValueError\u001b[39m: Missing column provided to 'parse_dates': 'Date'"
     ]
    }
   ],
   "source": [
    "# === データ取得 ===\n",
    "# ネット不可なら csv_path を指定して利用: e.g., './7203.T.csv'\n",
    "CSV_PATH = \"data/TM_1980-01-01_2025-06-27.csv\"\n",
    "TICKER = \"7203.T\"\n",
    "START_DATE = \"2020-09-01\"\n",
    "END_DATE = \"2025-09-30\"\n",
    "\n",
    "df = fetcher.fetch_ohlcv(\n",
    "    ticker=TICKER,\n",
    "    start=START_DATE,\n",
    "    end=END_DATE,\n",
    "    interval='1d',\n",
    "    csv_path=CSV_PATH  # ここを指定すればYahoo非依存\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "\n",
    "df = fetcher.fetch_ohlcv(\n",
    "    ticker=TICKER,\n",
    "    start=START_DATE,\n",
    "    end=END_DATE,\n",
    "    interval='1d',\n",
    "    csv_path=CSV_PATH\n",
    ")\n",
    "\n",
    "print(df.head())\n",
    "print(df.tail())\n",
    "print(df.describe())\n",
    "\n",
    "# # 保存（チュートリアル互換）\n",
    "df.to_pickle('df_ohlcv_7203T.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab6497e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "価格データ(df)が存在しません。先にデータ取得セルを実行するか df_ohlcv_7203T.pkl を用意してください。",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded cached OHLCV data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfallback_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33m価格データ(df)が存在しません。先にデータ取得セルを実行するか df_ohlcv_7203T.pkl を用意してください。\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m df = df[(df.index >= pd.to_datetime(START_DATE)) & (df.index <= pd.to_datetime(END_DATE))].copy()\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# === テクニカル指標・派生特徴量 ===\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: 価格データ(df)が存在しません。先にデータ取得セルを実行するか df_ohlcv_7203T.pkl を用意してください。"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# === 期間フィルタ（念のため） ===\n",
    "if 'df' not in globals():\n",
    "    fallback_path = Path('df_ohlcv_7203T.pkl')\n",
    "    if fallback_path.exists():\n",
    "        df = pd.read_pickle(fallback_path)\n",
    "        print(f\"Loaded cached OHLCV data from {fallback_path}\")\n",
    "    else:\n",
    "        raise RuntimeError('価格データ(df)が存在しません。先にデータ取得セルを実行するか df_ohlcv_7203T.pkl を用意してください。')\n",
    "\n",
    "df = df[(df.index >= pd.to_datetime(START_DATE)) & (df.index <= pd.to_datetime(END_DATE))].copy()\n",
    "\n",
    "# === テクニカル指標・派生特徴量 ===\n",
    "def rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "    ma_up = up.rolling(window=period, min_periods=period).mean()\n",
    "    ma_down = down.rolling(window=period, min_periods=period).mean()\n",
    "    rs = ma_up / (ma_down + 1e-9)\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def macd(series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    hist = macd_line - signal_line\n",
    "    return macd_line, signal_line, hist\n",
    "\n",
    "def bollinger(series, window=20, num_std=2):\n",
    "    ma = series.rolling(window=window, min_periods=window).mean()\n",
    "    std = series.rolling(window=window, min_periods=window).std()\n",
    "    upper = ma + num_std * std\n",
    "    lower = ma - num_std * std\n",
    "    width = (upper - lower) / (ma + 1e-9)\n",
    "    return ma, upper, lower, width\n",
    "\n",
    "# 主要な終値ベースで算出\n",
    "close = df['close']\n",
    "\n",
    "df['ret_1d'] = close.pct_change()\n",
    "df['ma_7']   = close.rolling(7).mean()\n",
    "df['ma_30']  = close.rolling(30).mean()\n",
    "df['ema_7']  = close.ewm(span=7, adjust=False).mean()\n",
    "df['ema_30'] = close.ewm(span=30, adjust=False).mean()\n",
    "df['rsi_14'] = rsi(close, 14)\n",
    "\n",
    "macd_line, signal_line, macd_hist = macd(close)\n",
    "df['macd']   = macd_line\n",
    "df['macd_s'] = signal_line\n",
    "df['macd_h'] = macd_hist\n",
    "\n",
    "bb_ma, bb_up, bb_lo, bb_w = bollinger(close, 20, 2)\n",
    "df['bb_ma'] = bb_ma\n",
    "df['bb_up'] = bb_up\n",
    "df['bb_lo'] = bb_lo\n",
    "df['bb_w']  = bb_w\n",
    "\n",
    "# 出来高系\n",
    "df['vol_chg'] = df['volume'].pct_change()\n",
    "\n",
    "# 1日先の終値（回帰ターゲット）\n",
    "df['target_close_t1'] = df['close'].shift(-1)\n",
    "\n",
    "# 1日先が上昇なら1（Buy）、下降なら0（Sell）\n",
    "df['target_buy'] = (df['target_close_t1'] > df['close']).astype(float)\n",
    "\n",
    "# 欠損除去\n",
    "df = df.dropna().copy()\n",
    "print('Final shape with features:', df.shape)\n",
    "\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af352d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 可視化（概観） ===\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "df['close'].plot(ax=ax)\n",
    "ax.set_title('Toyota (7203.T) Close')\n",
    "ax.grid(True)\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,3))\n",
    "df['rsi_14'].plot(ax=ax)\n",
    "ax.axhline(70, linestyle='--'); ax.axhline(30, linestyle='--')\n",
    "ax.set_title('RSI(14)')\n",
    "ax.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25dcc93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 時系列分割 ===\n",
    "train = df[df.index <= SPLIT_TRAIN_END].copy()\n",
    "val   = df[(df.index > SPLIT_TRAIN_END) & (df.index <= SPLIT_VAL_END)].copy()\n",
    "test  = df[df.index > SPLIT_VAL_END].copy()\n",
    "\n",
    "print('train:', train.index.min(), '->', train.index.max(), len(train))\n",
    "print('val  :', val.index.min(),   '->', val.index.max(),   len(val))\n",
    "print('test :', test.index.min(),  '->', test.index.max(),  len(test))\n",
    "\n",
    "# 特徴量カラム\n",
    "FEATURE_COLS = [\n",
    "    'open','high','low','close','adjclose','volume',\n",
    "    'ret_1d','ma_7','ma_30','ema_7','ema_30','rsi_14',\n",
    "    'macd','macd_s','macd_h','bb_ma','bb_up','bb_lo','bb_w','vol_chg'\n",
    "]\n",
    "\n",
    "def make_window_dataset(df_part, feature_cols, target_col, window):\n",
    "    X_list, y_list = [], []\n",
    "    feats = df_part[feature_cols].values\n",
    "    target = df_part[target_col].values\n",
    "    for i in range(len(df_part) - window):\n",
    "        X_list.append(feats[i:i+window])\n",
    "        y_list.append(target[i+window])\n",
    "    return np.array(X_list), np.array(y_list)\n",
    "\n",
    "# スケーラーは訓練でfitし、他でtransform\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[FEATURE_COLS].values)\n",
    "\n",
    "def scale_df(df_part):\n",
    "    cp = df_part.copy()\n",
    "    cp[FEATURE_COLS] = scaler.transform(cp[FEATURE_COLS].values)\n",
    "    return cp\n",
    "\n",
    "train_s = scale_df(train)\n",
    "val_s   = scale_df(val)\n",
    "test_s  = scale_df(test)\n",
    "\n",
    "# 回帰用データセット（翌日終値）\n",
    "X_train_reg, y_train_reg = make_window_dataset(train_s, FEATURE_COLS, 'target_close_t1', WINDOW_SIZE)\n",
    "X_val_reg,   y_val_reg   = make_window_dataset(val_s,   FEATURE_COLS, 'target_close_t1', WINDOW_SIZE)\n",
    "X_test_reg,  y_test_reg  = make_window_dataset(test_s,  FEATURE_COLS, 'target_close_t1', WINDOW_SIZE)\n",
    "\n",
    "# 分類用（Buy/Sell）\n",
    "X_train_cls, y_train_cls = make_window_dataset(train_s, FEATURE_COLS, 'target_buy', WINDOW_SIZE)\n",
    "X_val_cls,   y_val_cls   = make_window_dataset(val_s,   FEATURE_COLS, 'target_buy', WINDOW_SIZE)\n",
    "X_test_cls,  y_test_cls  = make_window_dataset(test_s,  FEATURE_COLS, 'target_buy', WINDOW_SIZE)\n",
    "\n",
    "X_train_reg.shape, X_val_reg.shape, X_test_reg.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6182d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === ベースライン：線形回帰（回帰） ===\n",
    "# ウィンドウを平均で潰して単純特徴に落とす簡易ベースライン\n",
    "def collapse_window_mean(X):\n",
    "    # (N, window, F) -> (N, F) by mean\n",
    "    return X.mean(axis=1)\n",
    "\n",
    "Xtr_bl = collapse_window_mean(X_train_reg)\n",
    "Xv_bl  = collapse_window_mean(X_val_reg)\n",
    "Xte_bl = collapse_window_mean(X_test_reg)\n",
    "\n",
    "linr = LinearRegression()\n",
    "linr.fit(Xtr_bl, y_train_reg)\n",
    "\n",
    "pred_tr_bl = linr.predict(Xtr_bl)\n",
    "pred_v_bl  = linr.predict(Xv_bl)\n",
    "pred_te_bl = linr.predict(Xte_bl)\n",
    "\n",
    "def rmse(y, p): return math.sqrt(mean_squared_error(y, p))\n",
    "\n",
    "print('Baseline Linear Regression')\n",
    "print('  Train RMSE:', rmse(y_train_reg, pred_tr_bl))\n",
    "print('  Val   RMSE:', rmse(y_val_reg,   pred_v_bl))\n",
    "print('  Test  RMSE:', rmse(y_test_reg,  pred_te_bl))\n",
    "\n",
    "print('  Test R2  :', r2_score(y_test_reg, pred_te_bl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec248b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === LSTM（回帰：翌日終値） ===\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model_reg = Sequential([\n",
    "    LSTM(64, return_sequences=True, input_shape=(X_train_reg.shape[1], X_train_reg.shape[2])),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64),\n",
    "    Dense(1, activation='linear')\n",
    "])\n",
    "model_reg.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True, monitor='val_loss'),\n",
    "    ReduceLROnPlateau(patience=5, factor=0.5, monitor='val_loss', verbose=1),\n",
    "    ModelCheckpoint('best_regression.keras', monitor='val_loss', save_best_only=True, verbose=0)\n",
    "]\n",
    "\n",
    "hist = model_reg.fit(\n",
    "    X_train_reg, y_train_reg,\n",
    "    validation_data=(X_val_reg, y_val_reg),\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 推論\n",
    "pred_tr = model_reg.predict(X_train_reg).ravel()\n",
    "pred_v  = model_reg.predict(X_val_reg).ravel()\n",
    "pred_te = model_reg.predict(X_test_reg).ravel()\n",
    "\n",
    "print('LSTM Regression')\n",
    "print('  Train RMSE:', rmse(y_train_reg, pred_tr))\n",
    "print('  Val   RMSE:', rmse(y_val_reg,   pred_v))\n",
    "print('  Test  RMSE:', rmse(y_test_reg,  pred_te))\n",
    "print('  Test R2  :', r2_score(y_test_reg, pred_te))\n",
    "\n",
    "# 学習曲線\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(hist.history['loss'], label='train')\n",
    "plt.plot(hist.history['val_loss'], label='val')\n",
    "plt.title('LSTM Regression Loss')\n",
    "plt.legend(); plt.grid(True); plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d5aeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 分類：予測終値に基づく Buy(1)/Sell(0) 判定 ===\n",
    "# 判定ロジック： pred_close_{t+1} > actual_close_{t} ? 1 : 0\n",
    "# 時系列整合のため、各セットの基準 day_t の close 実値を準備する\n",
    "\n",
    "def get_last_close_vector(df_part, window):\n",
    "    # 各サンプルの \"直近日の実Close\" を取り出す (ラベル生成用)\n",
    "    vals = df_part['close'].values  # *スケール前* が本来望ましいが、ここは y と比較するだけなので OK\n",
    "    # ウィンドウで切った最後の行の index を対応づけ\n",
    "    out = []\n",
    "    for i in range(len(df_part) - window):\n",
    "        out.append(vals[i + window - 1])\n",
    "    return np.array(out)\n",
    "\n",
    "close_train_tail = get_last_close_vector(train, WINDOW_SIZE)\n",
    "close_val_tail   = get_last_close_vector(val,   WINDOW_SIZE)\n",
    "close_test_tail  = get_last_close_vector(test,  WINDOW_SIZE)\n",
    "\n",
    "buy_pred_tr = (pred_tr > close_train_tail).astype(int)\n",
    "buy_pred_v  = (pred_v  > close_val_tail).astype(int)\n",
    "buy_pred_te = (pred_te > close_test_tail).astype(int)\n",
    "\n",
    "print('=== Classification Metrics (Buy=1 / Sell=0) ===')\n",
    "def cls_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        'acc': accuracy_score(y_true, y_pred),\n",
    "        'prec': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'rec': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "print('Train:', cls_metrics(y_train_cls, buy_pred_tr))\n",
    "print('Val  :', cls_metrics(y_val_cls,   buy_pred_v))\n",
    "print('Test :', cls_metrics(y_test_cls,  buy_pred_te))\n",
    "\n",
    "# Confusion Matrix (Test)\n",
    "cm = confusion_matrix(y_test_cls, buy_pred_te)\n",
    "print('\\nConfusion Matrix (Test)\\n', cm)\n",
    "print('\\nClassification Report (Test)\\n', classification_report(y_test_cls, buy_pred_te, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45899e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === シンプル・バックテスト（テスト区間のみ） ===\n",
    "# ルール：\n",
    "#   Buy(1) -> 翌日寄りで買って翌日引けで手仕舞い（= 翌日終値と当日終値の差分に連動すると仮定）\n",
    "#   Sell(0) -> 何もしない（空売り等は考慮しないシンプル版）\n",
    "\n",
    "test_close = test['close'].values\n",
    "# ウィンドウ切り詰めに合わせて test_close を末尾1日分削る（y_test_reg と長さ一致）\n",
    "test_close_tail = test_close[WINDOW_SIZE-1: -1]  # day_t close\n",
    "test_close_next = test_close[WINDOW_SIZE:]       # day_{t+1} close\n",
    "\n",
    "# 収益率（Buyのときのみリターンを計上）\n",
    "ret = np.zeros_like(buy_pred_te, dtype=float)\n",
    "price_diff = (test_close_next - test_close_tail) / (test_close_tail + 1e-9)  # 日次騰落率\n",
    "ret[buy_pred_te == 1] = price_diff[buy_pred_te == 1]\n",
    "\n",
    "cum_ret = (1 + ret).cumprod() - 1\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(cum_ret, label='Strategy (Buy on predicted up)')\n",
    "plt.axhline(0, color='k', linestyle='--')\n",
    "plt.title('Cumulative Return (Test Period)')\n",
    "plt.grid(True); plt.legend(); plt.show()\n",
    "\n",
    "print('Final cumulative return (test): {:.2%}'.format(cum_ret[-1] if len(cum_ret)>0 else 0.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa0a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === 保存物 ===\n",
    "joblib.dump(scaler, 'scaler_7203T.joblib')\n",
    "model_reg.save('model_regression_7203T.keras')\n",
    "\n",
    "with open('params_7203T.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(f'Ticker: {TICKER}\\n')\n",
    "    f.write(f'Period: {START_DATE}..{END_DATE}\\n')\n",
    "    f.write(f'Window: {WINDOW_SIZE}\\n')\n",
    "    f.write(f'Train end: {SPLIT_TRAIN_END}\\nVal end: {SPLIT_VAL_END}\\n')\n",
    "print('Artifacts saved.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29705a86",
   "metadata": {},
   "source": [
    "\n",
    "## メモ：クリプト向けチュートリアルからの置換ポイント\n",
    "- **GmoFetcher → YfFetcher**  \n",
    "  - 15分足 / `interval_sec` は株の**日次**に読み替え（`interval='1d'`）。  \n",
    "  - `market='BTC_JPY'` → `ticker='7203.T'`。  \n",
    "  - キャッシュは `joblib.Memory('/tmp/yf_cache')` を用意。\n",
    "\n",
    "- **データ期間の限定**  \n",
    "  - `df = df[df.index < '2021-04-01']` のような扱いを、`START_DATE`〜`END_DATE` で制御。\n",
    "\n",
    "- **目的変数**  \n",
    "  - 回帰：翌日終値 `target_close_t1`。  \n",
    "  - 分類：`pred_close_{t+1} > close_t` を Buy=1 / Sell=0。\n",
    "\n",
    "- **評価**  \n",
    "  - 回帰：RMSE / R²。  \n",
    "  - 分類：Accuracy / Precision / Recall / F1、Confusion Matrix。  \n",
    "  - 簡易バックテストで実務的な感触も確認。\n",
    "\n",
    "- **ネットワーク非依存**  \n",
    "  - ネット不可の環境では Kaggle 等で `7203.T.csv` を持ち込み、`csv_path` に指定。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}